---
tags:
  - Optimization
---
작은 미니배치에서 경사 하강법을 구현한다고 할 때 정확하게 최솟값에 멈추기 어려울 것이다. 
왜냐하면 항상 같은 학습률 $\alpha$ 를 가지고 있기 때문이다.

그러나 $\alpha$를 점점 작게하면 단계마다 진행정도가 작아지면서 최솟값 주변에서 작은 값으로 맴돌게 될 것이다. 따라 학습초반에는 큰 값으로 학습하고 점점 학습률을 줄이게 하는 방식을 의미한다.


- 수식
$$
\alpha = \frac{1}{(1+decay\ rate) * epoch\ num} * \alpha_{-1}
$$



예시로 $\alpha_0$이 0.2이고, decay rate가 1일 때 아래와 같이 변화할 수 있다.

| epoch | learning rate |
| ----- | ------------- |
| 1     | 0.1           |
| 2     | 0.67          |
| 3     | 0.5           |
| 4     | 0.4              |
